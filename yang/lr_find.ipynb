{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.imports import *\n",
    "from input_fn import input_fn\n",
    "from cvt2tfrecord import fn_record_to_count\n",
    "from utils.clr import LRFinder, OneCycleLR\n",
    "\n",
    "# from tensorflow.keras.layers import *\n",
    "# import tensorflow.keras as keras\n",
    "# from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # so the IDs match nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # \"0, 1\" for multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_factory import ModelType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.transforms import *\n",
    "PATH = Path('../data')\n",
    "sz = 128\n",
    "# sz = 256\n",
    "nt = 10\n",
    "bs = 12\n",
    "MODEL_VERSION = 'prednet_' + str(sz) + '_1'\n",
    "MODEL_PATH = PATH/'models'\n",
    "\n",
    "num_gpus = 1\n",
    "\n",
    "# class Slice(Transform):\n",
    "#     \"\"\" Return a slice of the images\n",
    "    \n",
    "#     Arguments:\n",
    "#     The same as the built-in function slice\n",
    "#     \"\"\"\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         self.slice = slice(*args, **kwargs)\n",
    "#         super().__init__(TfmType.NO)\n",
    "        \n",
    "#     def do_transform(self, x, is_y):\n",
    "#         return x[self.slice]\n",
    "\n",
    "# aug_tfms = [Slice(nt)]\n",
    "aug_tfms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.prednet_refactored import PredNetCell, PredNet\n",
    "\n",
    "# n_channels, im_height, im_width = (3, 128, 160)\n",
    "n_channels, im_height, im_width = (1, sz, sz)\n",
    "input_shape = (im_height, im_width, n_channels)\n",
    "stack_sizes = (n_channels, 48, 96, 192)\n",
    "R_stack_sizes = stack_sizes\n",
    "A_filt_sizes = (3, 3, 3)\n",
    "Ahat_filt_sizes = (3, 3, 3, 3)\n",
    "R_filt_sizes = (3, 3, 3, 3)\n",
    "\n",
    "layer_loss_weights = np.array([1., 0., 0., 0.])  # weighting for each layer in final loss; \"L_0\" model:  [1, 0, 0, 0], \"L_all\": [1, 0.1, 0.1, 0.1]\n",
    "layer_loss_weights = np.expand_dims(layer_loss_weights, 1)\n",
    "time_loss_weights = 1./ (nt - 1) * np.ones((nt,1))  # equally weight all timesteps except the first\n",
    "time_loss_weights[0] = 0\n",
    "\n",
    "prednet_cell = PredNetCell(stack_sizes=stack_sizes,\n",
    "                    R_stack_sizes=R_stack_sizes,\n",
    "                    A_filt_sizes=A_filt_sizes,\n",
    "                    Ahat_filt_sizes=Ahat_filt_sizes,\n",
    "                    R_filt_sizes=R_filt_sizes)\n",
    "\n",
    "prednet = PredNet(prednet_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10, 128, 128, 1)   0         \n",
      "_________________________________________________________________\n",
      "pred_net (PredNet)           (None, 10, 4)             6909818   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 1)             5         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,909,834\n",
      "Trainable params: 6,909,818\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "inputs = tf.keras.Input(shape=(nt,) + input_shape)\n",
    "errors = prednet(inputs)  # errors will be (batch_size, nt, nb_layers) \n",
    "errors_by_time = TimeDistributed(Dense(1, trainable=False), weights=[layer_loss_weights, np.zeros(1)], trainable=False)(errors)  # calculate weighted error by layer\n",
    "errors_by_time = Flatten()(errors_by_time)  # will be (batch_size, nt)\n",
    "final_errors = Dense(1, weights=[time_loss_weights, np.zeros(1)], trainable=False)(errors_by_time)  # weight errors by time\n",
    "model = Model(inputs=inputs, outputs=final_errors)\n",
    "# model = tf.keras.utils.multi_gpu_model(model, gpus=2)\n",
    "optimizer = 'adam'\n",
    "optimizer = 'sgd'\n",
    "optimizer = SGD(lr=0.002, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = ['../data/tfrecords/train_1_contiguous_10']\n",
    "val_split = 0.1\n",
    "num_parallel_calls=32\n",
    "buffer_size = 3\n",
    "x, y, num_samples             = input_fn(bs, sz, nt, aug_tfms, fns, is_val=False, val_split=val_split,\n",
    "                                         stats_fn='stat.csv', stats_sep=',', num_parallel_calls=num_parallel_calls, shuffle=True,\n",
    "                                         buffer_size=buffer_size)\n",
    "val_x, val_y, val_num_samples = input_fn(bs, sz, nt, aug_tfms, fns, is_val=True, val_split=val_split,\n",
    "                                         stats_fn='stat.csv', stats_sep=',', num_parallel_calls=num_parallel_calls, shuffle=False,\n",
    "                                         buffer_size=buffer_size)\n",
    "# num_iterations is pre-batch iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general training configs\n",
    "path_checkpoints = MODEL_PATH/'checkpoints'\n",
    "path_lrs = MODEL_PATH/'lrs'/MODEL_VERSION\n",
    "if not path_checkpoints.exists(): path_checkpoints.mkdir(parents=True)\n",
    "model_callbacks = [\n",
    "    # TODO: save info of prednet cell\n",
    "    tf.keras.callbacks.TensorBoard(),\n",
    "#     keras.callbacks.History(),\n",
    "#     tf.keras.callbacks.ModelCheckpoint(str(path_checkpoints/('weights.' + MODEL_VERSION + '.{epoch:02d}-{val_loss:.2f}.hdf5'))),\n",
    "    tf.keras.callbacks.ModelCheckpoint(str(path_checkpoints/('weights.' + MODEL_VERSION + '.{epoch:02d}.hdf5'))),\n",
    "#     keras.callbacks.EarlyStopping()\n",
    "]\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pct = 0.01\n",
    "lr_num_samples = num_samples * lr_pct\n",
    "lr_steps = int(epochs * lr_num_samples / bs)\n",
    "\n",
    "lrfinder = LRFinder(lr_num_samples, bs, save_dir=str(path_checkpoints))\n",
    "lr_callbacks = [lrfinder]\n",
    "# callbacks = model_callbacks + lr_callbacks\n",
    "callbacks = lr_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - LRFinder: lr = 0.00001064 \n",
      "  1/225 [..............................] - ETA: 20:26 - loss: 0.3510 - LRFinder: lr = 0.00001131 \n",
      "  2/225 [..............................] - ETA: 11:25 - loss: 0.3600 - LRFinder: lr = 0.00001203 \n",
      "  3/225 [..............................] - ETA: 8:24 - loss: 0.3591  - LRFinder: lr = 0.00001280 \n",
      "  4/225 [..............................] - ETA: 6:53 - loss: 0.3578 - LRFinder: lr = 0.00001361 \n",
      "  5/225 [..............................] - ETA: 5:58 - loss: 0.3529 - LRFinder: lr = 0.00001448 \n",
      "  6/225 [..............................] - ETA: 5:22 - loss: 0.3473 - LRFinder: lr = 0.00001540 \n",
      "  7/225 [..............................] - ETA: 4:56 - loss: 0.3447 - LRFinder: lr = 0.00001638 \n",
      "  8/225 [>.............................] - ETA: 4:36 - loss: 0.3454 - LRFinder: lr = 0.00001742 \n",
      "  9/225 [>.............................] - ETA: 4:21 - loss: 0.3463 - LRFinder: lr = 0.00001853 \n",
      " 10/225 [>.............................] - ETA: 4:08 - loss: 0.3463 - LRFinder: lr = 0.00001971 \n",
      " 11/225 [>.............................] - ETA: 3:58 - loss: 0.3455 - LRFinder: lr = 0.00002096 \n",
      " 12/225 [>.............................] - ETA: 3:49 - loss: 0.3451 - LRFinder: lr = 0.00002230 \n",
      " 13/225 [>.............................] - ETA: 3:41 - loss: 0.3465 - LRFinder: lr = 0.00002371 \n",
      " 14/225 [>.............................] - ETA: 3:35 - loss: 0.3488 - LRFinder: lr = 0.00002522 \n",
      " 15/225 [=>............................] - ETA: 3:30 - loss: 0.3495 - LRFinder: lr = 0.00002683 \n",
      " 16/225 [=>............................] - ETA: 3:25 - loss: 0.3503 - LRFinder: lr = 0.00002853 \n",
      " 17/225 [=>............................] - ETA: 3:20 - loss: 0.3514 - LRFinder: lr = 0.00003035 \n",
      " 18/225 [=>............................] - ETA: 3:16 - loss: 0.3520 - LRFinder: lr = 0.00003228 \n",
      " 19/225 [=>............................] - ETA: 3:12 - loss: 0.3521 - LRFinder: lr = 0.00003433 \n",
      " 20/225 [=>............................] - ETA: 3:09 - loss: 0.3531 - LRFinder: lr = 0.00003652 \n",
      " 21/225 [=>............................] - ETA: 3:05 - loss: 0.3543 - LRFinder: lr = 0.00003884 \n",
      " 22/225 [=>............................] - ETA: 3:02 - loss: 0.3548 - LRFinder: lr = 0.00004131 \n",
      " 23/225 [==>...........................] - ETA: 3:00 - loss: 0.3561 - LRFinder: lr = 0.00004394 \n",
      " 24/225 [==>...........................] - ETA: 2:57 - loss: 0.3572 - LRFinder: lr = 0.00004674 \n",
      " 25/225 [==>...........................] - ETA: 2:55 - loss: 0.3579 - LRFinder: lr = 0.00004971 \n",
      " 26/225 [==>...........................] - ETA: 2:53 - loss: 0.3588 - LRFinder: lr = 0.00005287 \n",
      " 27/225 [==>...........................] - ETA: 2:50 - loss: 0.3591 - LRFinder: lr = 0.00005623 \n",
      " 28/225 [==>...........................] - ETA: 2:48 - loss: 0.3596 - LRFinder: lr = 0.00005981 \n",
      " 29/225 [==>...........................] - ETA: 2:46 - loss: 0.3595 - LRFinder: lr = 0.00006362 \n",
      " 30/225 [===>..........................] - ETA: 2:44 - loss: 0.3592 - LRFinder: lr = 0.00006766 \n",
      " 31/225 [===>..........................] - ETA: 2:42 - loss: 0.3589 - LRFinder: lr = 0.00007197 \n",
      " 32/225 [===>..........................] - ETA: 2:41 - loss: 0.3592 - LRFinder: lr = 0.00007655 \n",
      " 33/225 [===>..........................] - ETA: 2:39 - loss: 0.3591 - LRFinder: lr = 0.00008142 \n",
      " 34/225 [===>..........................] - ETA: 2:37 - loss: 0.3590 - LRFinder: lr = 0.00008660 \n",
      " 35/225 [===>..........................] - ETA: 2:36 - loss: 0.3588 - LRFinder: lr = 0.00009211 \n",
      " 36/225 [===>..........................] - ETA: 2:34 - loss: 0.3588 - LRFinder: lr = 0.00009797 \n",
      " 37/225 [===>..........................] - ETA: 2:33 - loss: 0.3583 - LRFinder: lr = 0.00010420 \n",
      " 38/225 [====>.........................] - ETA: 2:32 - loss: 0.3584 - LRFinder: lr = 0.00011083 \n",
      " 39/225 [====>.........................] - ETA: 2:31 - loss: 0.3583 - LRFinder: lr = 0.00011788 \n",
      " 40/225 [====>.........................] - ETA: 2:29 - loss: 0.3580 - LRFinder: lr = 0.00012538 \n",
      " 41/225 [====>.........................] - ETA: 2:28 - loss: 0.3576 - LRFinder: lr = 0.00013335 \n",
      " 42/225 [====>.........................] - ETA: 2:27 - loss: 0.3575 - LRFinder: lr = 0.00014184 \n",
      " 43/225 [====>.........................] - ETA: 2:25 - loss: 0.3574 - LRFinder: lr = 0.00015086 \n",
      " 44/225 [====>.........................] - ETA: 2:24 - loss: 0.3573 - LRFinder: lr = 0.00016046 \n",
      " 45/225 [=====>........................] - ETA: 2:23 - loss: 0.3571 - LRFinder: lr = 0.00017066 \n",
      " 46/225 [=====>........................] - ETA: 2:22 - loss: 0.3567 - LRFinder: lr = 0.00018152 \n",
      " 47/225 [=====>........................] - ETA: 2:20 - loss: 0.3561 - LRFinder: lr = 0.00019307 \n",
      " 48/225 [=====>........................] - ETA: 2:19 - loss: 0.3557 - LRFinder: lr = 0.00020535 \n",
      " 49/225 [=====>........................] - ETA: 2:18 - loss: 0.3550 - LRFinder: lr = 0.00021842 \n",
      " 50/225 [=====>........................] - ETA: 2:17 - loss: 0.3546 - LRFinder: lr = 0.00023231 \n",
      " 51/225 [=====>........................] - ETA: 2:16 - loss: 0.3543 - LRFinder: lr = 0.00024709 \n",
      " 52/225 [=====>........................] - ETA: 2:15 - loss: 0.3541 - LRFinder: lr = 0.00026281 \n",
      " 53/225 [======>.......................] - ETA: 2:14 - loss: 0.3538 - LRFinder: lr = 0.00027953 \n",
      " 54/225 [======>.......................] - ETA: 2:12 - loss: 0.3533 - LRFinder: lr = 0.00029731 \n",
      " 55/225 [======>.......................] - ETA: 2:11 - loss: 0.3530 - LRFinder: lr = 0.00031623 \n",
      " 56/225 [======>.......................] - ETA: 2:10 - loss: 0.3528 - LRFinder: lr = 0.00033635 \n",
      " 57/225 [======>.......................] - ETA: 2:09 - loss: 0.3532 - LRFinder: lr = 0.00035774 \n",
      " 58/225 [======>.......................] - ETA: 2:08 - loss: 0.3530 - LRFinder: lr = 0.00038050 \n",
      " 59/225 [======>.......................] - ETA: 2:07 - loss: 0.3527 - LRFinder: lr = 0.00040471 \n",
      " 60/225 [=======>......................] - ETA: 2:06 - loss: 0.3525 - LRFinder: lr = 0.00043046 \n",
      " 61/225 [=======>......................] - ETA: 2:05 - loss: 0.3521 - LRFinder: lr = 0.00045784 \n",
      " 62/225 [=======>......................] - ETA: 2:04 - loss: 0.3518 - LRFinder: lr = 0.00048697 \n",
      " 63/225 [=======>......................] - ETA: 2:03 - loss: 0.3519 - LRFinder: lr = 0.00051795 \n",
      " 64/225 [=======>......................] - ETA: 2:02 - loss: 0.3522 - LRFinder: lr = 0.00055090 \n",
      " 65/225 [=======>......................] - ETA: 2:01 - loss: 0.3522 - LRFinder: lr = 0.00058595 \n",
      " 66/225 [=======>......................] - ETA: 2:01 - loss: 0.3524 - LRFinder: lr = 0.00062322 \n",
      " 67/225 [=======>......................] - ETA: 2:00 - loss: 0.3526 - LRFinder: lr = 0.00066287 \n",
      " 68/225 [========>.....................] - ETA: 1:59 - loss: 0.3529 - LRFinder: lr = 0.00070504 \n",
      " 69/225 [========>.....................] - ETA: 1:58 - loss: 0.3532 - LRFinder: lr = 0.00074989 \n",
      " 70/225 [========>.....................] - ETA: 1:57 - loss: 0.3533 - LRFinder: lr = 0.00079760 \n",
      " 71/225 [========>.....................] - ETA: 1:56 - loss: 0.3529 - LRFinder: lr = 0.00084834 \n",
      " 72/225 [========>.....................] - ETA: 1:55 - loss: 0.3526 - LRFinder: lr = 0.00090231 \n",
      " 73/225 [========>.....................] - ETA: 1:55 - loss: 0.3519 - LRFinder: lr = 0.00095972 \n",
      " 74/225 [========>.....................] - ETA: 1:54 - loss: 0.3513 - LRFinder: lr = 0.00102077 \n",
      " 75/225 [=========>....................] - ETA: 1:53 - loss: 0.3511 - LRFinder: lr = 0.00108571 \n",
      " 76/225 [=========>....................] - ETA: 1:52 - loss: 0.3508 - LRFinder: lr = 0.00115478 \n",
      " 77/225 [=========>....................] - ETA: 1:51 - loss: 0.3504 - LRFinder: lr = 0.00122825 \n",
      " 78/225 [=========>....................] - ETA: 1:50 - loss: 0.3500 - LRFinder: lr = 0.00130639 \n",
      " 79/225 [=========>....................] - ETA: 1:49 - loss: 0.3496 - LRFinder: lr = 0.00138950 \n",
      " 80/225 [=========>....................] - ETA: 1:49 - loss: 0.3491 - LRFinder: lr = 0.00147789 \n",
      " 81/225 [=========>....................] - ETA: 1:48 - loss: 0.3488 - LRFinder: lr = 0.00157191 \n",
      " 82/225 [=========>....................] - ETA: 1:47 - loss: 0.3488 - LRFinder: lr = 0.00167192 \n",
      " 83/225 [==========>...................] - ETA: 1:46 - loss: 0.3481 - LRFinder: lr = 0.00177828 \n",
      " 84/225 [==========>...................] - ETA: 1:45 - loss: 0.3476 - LRFinder: lr = 0.00189141 \n",
      " 85/225 [==========>...................] - ETA: 1:45 - loss: 0.3471 - LRFinder: lr = 0.00201174 \n",
      " 86/225 [==========>...................] - ETA: 1:44 - loss: 0.3467 - LRFinder: lr = 0.00213972 \n",
      " 87/225 [==========>...................] - ETA: 1:43 - loss: 0.3464 - LRFinder: lr = 0.00227585 \n",
      " 88/225 [==========>...................] - ETA: 1:42 - loss: 0.3462 - LRFinder: lr = 0.00242063 \n",
      " 89/225 [==========>...................] - ETA: 1:41 - loss: 0.3458 - LRFinder: lr = 0.00257463 \n",
      " 90/225 [===========>..................] - ETA: 1:41 - loss: 0.3454 - LRFinder: lr = 0.00273842 \n",
      " 91/225 [===========>..................] - ETA: 1:40 - loss: 0.3450 - LRFinder: lr = 0.00291263 \n",
      " 92/225 [===========>..................] - ETA: 1:39 - loss: 0.3446 - LRFinder: lr = 0.00309793 \n",
      " 93/225 [===========>..................] - ETA: 1:38 - loss: 0.3441 - LRFinder: lr = 0.00329501 \n",
      " 94/225 [===========>..................] - ETA: 1:37 - loss: 0.3436 - LRFinder: lr = 0.00350464 \n",
      " 95/225 [===========>..................] - ETA: 1:37 - loss: 0.3432 - LRFinder: lr = 0.00372759 \n",
      " 96/225 [===========>..................] - ETA: 1:36 - loss: 0.3427 - LRFinder: lr = 0.00396474 \n",
      " 97/225 [===========>..................] - ETA: 1:35 - loss: 0.3422 - LRFinder: lr = 0.00421696 \n",
      " 98/225 [============>.................] - ETA: 1:34 - loss: 0.3417 - LRFinder: lr = 0.00448524 \n",
      " 99/225 [============>.................] - ETA: 1:34 - loss: 0.3411 - LRFinder: lr = 0.00477058 \n",
      "100/225 [============>.................] - ETA: 1:33 - loss: 0.3405 - LRFinder: lr = 0.00507408 \n",
      "101/225 [============>.................] - ETA: 1:32 - loss: 0.3398 - LRFinder: lr = 0.00539688 \n",
      "102/225 [============>.................] - ETA: 1:31 - loss: 0.3390 - LRFinder: lr = 0.00574022 \n",
      "103/225 [============>.................] - ETA: 1:31 - loss: 0.3382 - LRFinder: lr = 0.00610540 \n",
      "104/225 [============>.................] - ETA: 1:30 - loss: 0.3374 - LRFinder: lr = 0.00649382 \n",
      "105/225 [=============>................] - ETA: 1:29 - loss: 0.3364 - LRFinder: lr = 0.00690694 \n",
      "106/225 [=============>................] - ETA: 1:28 - loss: 0.3354 - LRFinder: lr = 0.00734635 \n",
      "107/225 [=============>................] - ETA: 1:27 - loss: 0.3343 - LRFinder: lr = 0.00781371 \n",
      "108/225 [=============>................] - ETA: 1:27 - loss: 0.3332 - LRFinder: lr = 0.00831080 \n",
      "109/225 [=============>................] - ETA: 1:26 - loss: 0.3319 - LRFinder: lr = 0.00883952 \n",
      "110/225 [=============>................] - ETA: 1:25 - loss: 0.3306 - LRFinder: lr = 0.00940187 \n",
      "111/225 [=============>................] - ETA: 1:24 - loss: 0.3291 - LRFinder: lr = 0.01000000 \n",
      "112/225 [=============>................] - ETA: 1:24 - loss: 0.3276 - LRFinder: lr = 0.01063618 \n",
      "113/225 [==============>...............] - ETA: 1:23 - loss: 0.3260 - LRFinder: lr = 0.01131283 \n",
      "114/225 [==============>...............] - ETA: 1:22 - loss: 0.3245 - LRFinder: lr = 0.01203253 \n",
      "115/225 [==============>...............] - ETA: 1:21 - loss: 0.3230 - LRFinder: lr = 0.01279802 \n",
      "116/225 [==============>...............] - ETA: 1:20 - loss: 0.3215 - LRFinder: lr = 0.01361221 \n",
      "117/225 [==============>...............] - ETA: 1:20 - loss: 0.3200 - LRFinder: lr = 0.01447819 \n",
      "118/225 [==============>...............] - ETA: 1:19 - loss: 0.3180 - LRFinder: lr = 0.01539926 \n",
      "119/225 [==============>...............] - ETA: 1:18 - loss: 0.3160 - LRFinder: lr = 0.01637893 \n",
      "120/225 [===============>..............] - ETA: 1:17 - loss: 0.3140 - LRFinder: lr = 0.01742093 \n",
      "121/225 [===============>..............] - ETA: 1:17 - loss: 0.3115 - LRFinder: lr = 0.01852922 \n",
      "122/225 [===============>..............] - ETA: 1:16 - loss: 0.3091 - LRFinder: lr = 0.01970801 \n",
      "123/225 [===============>..............] - ETA: 1:15 - loss: 0.3067 - LRFinder: lr = 0.02096180 \n",
      "124/225 [===============>..............] - ETA: 1:14 - loss: 0.3044 - LRFinder: lr = 0.02229535 \n",
      "125/225 [===============>..............] - ETA: 1:14 - loss: 0.3021 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0135)\n",
      "126/225 [===============>..............] - ETA: 1:13 - loss: 0.3004 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0135)\n",
      "127/225 [===============>..............] - ETA: 1:12 - loss: 0.2984 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0135)\n",
      "128/225 [================>.............] - ETA: 1:11 - loss: 0.2965 - LRFinder: lr = 0.02371374 \n",
      "129/225 [================>.............] - ETA: 1:11 - loss: 0.2945 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0135)\n",
      "130/225 [================>.............] - ETA: 1:10 - loss: 0.2927 - LRFinder: lr = 0.02522236 \n",
      "131/225 [================>.............] - ETA: 1:09 - loss: 0.2907 - LRFinder: lr = 0.02682696 \n",
      "132/225 [================>.............] - ETA: 1:08 - loss: 0.2887 - LRFinder: lr = 0.02853364 \n",
      "133/225 [================>.............] - ETA: 1:08 - loss: 0.2866 - LRFinder: lr = 0.03034889 \n",
      "134/225 [================>.............] - ETA: 1:07 - loss: 0.2845 - LRFinder: lr = 0.03227963 \n",
      "135/225 [=================>............] - ETA: 1:06 - loss: 0.2824 - LRFinder: lr = 0.03433320 \n",
      "136/225 [=================>............] - ETA: 1:05 - loss: 0.2804 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "137/225 [=================>............] - ETA: 1:05 - loss: 0.2785 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "138/225 [=================>............] - ETA: 1:04 - loss: 0.2776 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "139/225 [=================>............] - ETA: 1:03 - loss: 0.2766 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "140/225 [=================>............] - ETA: 1:02 - loss: 0.2751 - LRFinder: lr = 0.03651742 \n",
      "141/225 [=================>............] - ETA: 1:02 - loss: 0.2732 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "142/225 [=================>............] - ETA: 1:01 - loss: 0.2715 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "143/225 [==================>...........] - ETA: 1:00 - loss: 0.2697 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "144/225 [==================>...........] - ETA: 59s - loss: 0.2680  - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "145/225 [==================>...........] - ETA: 59s - loss: 0.2664 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "146/225 [==================>...........] - ETA: 58s - loss: 0.2647 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "147/225 [==================>...........] - ETA: 57s - loss: 0.2631 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "148/225 [==================>...........] - ETA: 56s - loss: 0.2616 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "149/225 [==================>...........] - ETA: 56s - loss: 0.2601 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "150/225 [===================>..........] - ETA: 55s - loss: 0.2586 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "151/225 [===================>..........] - ETA: 54s - loss: 0.2571 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "152/225 [===================>..........] - ETA: 53s - loss: 0.2556 - LRFinder: lr = 0.03884058 \n",
      "153/225 [===================>..........] - ETA: 52s - loss: 0.2540 - LRFinder: lr = 0.04131155 \n",
      "154/225 [===================>..........] - ETA: 52s - loss: 0.2525 - LRFinder: lr = 0.04393971 \n",
      "155/225 [===================>..........] - ETA: 51s - loss: 0.2509 - LRFinder: lr = 0.04673507 \n",
      "156/225 [===================>..........] - ETA: 50s - loss: 0.2494 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "157/225 [===================>..........] - ETA: 49s - loss: 0.2481 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "158/225 [====================>.........] - ETA: 49s - loss: 0.2472 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "159/225 [====================>.........] - ETA: 48s - loss: 0.2463 - LRFinder: lr = 0.04970827 \n",
      "160/225 [====================>.........] - ETA: 47s - loss: 0.2448 - LRFinder: lr = 0.05287061 \n",
      "161/225 [====================>.........] - ETA: 46s - loss: 0.2434 - LRFinder: lr = 0.05623414 \n",
      "162/225 [====================>.........] - ETA: 46s - loss: 0.2420 - LRFinder: lr = 0.05981164 \n",
      "163/225 [====================>.........] - ETA: 45s - loss: 0.2406 - LRFinder: lr = 0.06361675 \n",
      "164/225 [====================>.........] - ETA: 44s - loss: 0.2392 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "165/225 [=====================>........] - ETA: 44s - loss: 0.2381 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "166/225 [=====================>........] - ETA: 43s - loss: 0.2370 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "167/225 [=====================>........] - ETA: 42s - loss: 0.2357 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "168/225 [=====================>........] - ETA: 41s - loss: 0.2345 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "169/225 [=====================>........] - ETA: 41s - loss: 0.2333 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "170/225 [=====================>........] - ETA: 40s - loss: 0.2320 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "171/225 [=====================>........] - ETA: 39s - loss: 0.2308 - LRFinder: lr = 0.06766392 \n",
      "172/225 [=====================>........] - ETA: 38s - loss: 0.2295 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "173/225 [======================>.......] - ETA: 38s - loss: 0.2283 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "174/225 [======================>.......] - ETA: 37s - loss: 0.2272 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "175/225 [======================>.......] - ETA: 36s - loss: 0.2262 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "176/225 [======================>.......] - ETA: 35s - loss: 0.2252 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "177/225 [======================>.......] - ETA: 35s - loss: 0.2242 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "178/225 [======================>.......] - ETA: 34s - loss: 0.2231 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "179/225 [======================>.......] - ETA: 33s - loss: 0.2220 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "180/225 [=======================>......] - ETA: 32s - loss: 0.2209 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "181/225 [=======================>......] - ETA: 32s - loss: 0.2198 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "182/225 [=======================>......] - ETA: 31s - loss: 0.2187 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "183/225 [=======================>......] - ETA: 30s - loss: 0.2177 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "184/225 [=======================>......] - ETA: 29s - loss: 0.2168 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "185/225 [=======================>......] - ETA: 29s - loss: 0.2158 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "186/225 [=======================>......] - ETA: 28s - loss: 0.2149 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "187/225 [=======================>......] - ETA: 27s - loss: 0.2140 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "188/225 [========================>.....] - ETA: 27s - loss: 0.2130 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "189/225 [========================>.....] - ETA: 26s - loss: 0.2120 - LRFinder: lr = 0.07196857 \n",
      "190/225 [========================>.....] - ETA: 25s - loss: 0.2110 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "191/225 [========================>.....] - ETA: 24s - loss: 0.2100 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "192/225 [========================>.....] - ETA: 24s - loss: 0.2090 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "193/225 [========================>.....] - ETA: 23s - loss: 0.2081 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "194/225 [========================>.....] - ETA: 22s - loss: 0.2071 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "195/225 [=========================>....] - ETA: 21s - loss: 0.2062 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "196/225 [=========================>....] - ETA: 21s - loss: 0.2053 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "197/225 [=========================>....] - ETA: 20s - loss: 0.2045 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "198/225 [=========================>....] - ETA: 19s - loss: 0.2036 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "199/225 [=========================>....] - ETA: 19s - loss: 0.2027 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "200/225 [=========================>....] - ETA: 18s - loss: 0.2019 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "201/225 [=========================>....] - ETA: 17s - loss: 0.2011 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "202/225 [=========================>....] - ETA: 16s - loss: 0.2003 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "203/225 [==========================>...] - ETA: 16s - loss: 0.1994 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "204/225 [==========================>...] - ETA: 15s - loss: 0.1986 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "205/225 [==========================>...] - ETA: 14s - loss: 0.1978 - LRFinder: lr = 0.07654707 \n",
      "206/225 [==========================>...] - ETA: 13s - loss: 0.1969 - LRFinder: lr = 0.08141685 \n",
      "207/225 [==========================>...] - ETA: 13s - loss: 0.1960 - LRFinder: lr = 0.08659644 \n",
      "208/225 [==========================>...] - ETA: 12s - loss: 0.1952 - LRFinder: lr = 0.09210554 \n",
      "209/225 [==========================>...] - ETA: 11s - loss: 0.1943 - LRFinder: lr = 0.09796512 \n",
      "210/225 [===========================>..] - ETA: 10s - loss: 0.1934 - LRFinder: lr = 0.10419747 \n",
      "211/225 [===========================>..] - ETA: 10s - loss: 0.1925 - LRFinder: lr = 0.11082631 \n",
      "212/225 [===========================>..] - ETA: 9s - loss: 0.1917  - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "213/225 [===========================>..] - ETA: 8s - loss: 0.1909 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "214/225 [===========================>..] - ETA: 8s - loss: 0.1901 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "215/225 [===========================>..] - ETA: 7s - loss: 0.1893 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "216/225 [===========================>..] - ETA: 6s - loss: 0.1886 - LRFinder: lr = 0.11787686 \n",
      "217/225 [===========================>..] - ETA: 5s - loss: 0.1878 - LRFinder: lr = 0.12537597 \n",
      "218/225 [============================>.] - ETA: 5s - loss: 0.1870 - LRFinder: lr = 0.13335215 \n",
      "219/225 [============================>.] - ETA: 4s - loss: 0.1862 - LRFinder: lr = 0.14183575 \n",
      "220/225 [============================>.] - ETA: 3s - loss: 0.1854 - LRFinder: lr = 0.15085907 \n",
      "221/225 [============================>.] - ETA: 2s - loss: 0.1846 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "222/225 [============================>.] - ETA: 2s - loss: 0.1840 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "223/225 [============================>.] - ETA: 1s - loss: 0.1834 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (0.0048)\n",
      "224/225 [============================>.] - ETA: 0s - loss: 0.1827 - LRFinder: lr = 0.16045644 \n",
      "\tLR Finder : Saved the losses and learning rate values in path : {../data/models/checkpoints}\n",
      "225/225 [==============================] - 164s 730ms/step - loss: 0.1819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff28bffaa20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, steps_per_epoch=lr_steps, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrfinder.plot_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 1e-3 / 2\n",
    "lr_manager = OneCycleLR(lr_num_samples, 1, bs, max_lr,\n",
    "                        end_percentage=0.1, scale_percentage=None,\n",
    "                        maximum_momentum=0.95, minimum_momentum=0.85, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "224/225 [============================>.] - ETA: 0s - loss: 0.2576 - lr: 0.00000 - momentum: 0.95 \n",
      "225/225 [==============================] - 216s 961ms/step - loss: 0.2572 - val_loss: 0.1746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3b41aa1b38>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_callbacks = [lr_manager]\n",
    "callbacks = model_callbacks + lr_callbacks\n",
    "\n",
    "model.fit(x, y, epochs=1, validation_data=(val_x, val_y), callbacks=callbacks, steps_per_epoch=lr_steps, validation_steps=lr_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
